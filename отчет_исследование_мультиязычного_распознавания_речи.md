# Отчет: Исследование решений для мультиязычного стримингового распознавания речи (Русский/Английский) на Linux Ubuntu с поддержкой GPU

## Введение

В данном отчете представлены результаты исследования современных библиотек и решений для создания мультиязычного приложения преобразования речи в текст с поддержкой стриминга, ориентированного на работу с русско-английской языковой смесью. Особое внимание уделено решениям, поддерживающим ускорение на GPU и имеющим бэкенд-архитектуру для интеграции в корпоративные приложения.

## Критерии оценки

1. **Поддержка мультиязычности** - возможность распознавания русской и английской речи
2. **Стриминговая обработка** - возможность реального времени или near-real-time обработки
3. **Поддержка GPU** - ускорение вычислений на видеокартах
4. **Бэкенд-интеграция** - наличие API для встраивания в приложения
5. **Производительность** - скорость и точность распознавания
6. **Лицензия** - открытость и коммерческая доступность

## Анализ библиотек и решений

### 1. OpenAI Whisper (Python/C++)

**Общая информация:**
- **Разработчик:** OpenAI
- **Лицензия:** MIT
- **Языки:** 99 языков включая русский и английский
- **Модели:** tiny (39M) → large-v3-turbo (798M)

**Преимущества:**
- Высокая точность распознавания для обоих языков
- Отличная поддержка смешанной речи (code-switching)
- Наличие предобученных моделей
- Активное сообщество и постоянные обновления

**GPU поддержка:**
```bash
# Установка с поддержкой CUDA
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install openai-whisper

# Использование с GPU
import whisper
model = whisper.load_model("large-v3", device="cuda")
result = model.transcribe("audio.mp3", language="ru")  # или "en"
```

**Стриминговые возможности:**
- Базовая реализация не поддерживает настоящий стриминг
- Требуется реализация чанковой обработки
- Пример стриминговой реализации:

```python
import whisper
import numpy as np
import sounddevice as sd

model = whisper.load_model("base", device="cuda")

def audio_callback(indata, frames, time, status):
    if status:
        print(status)
    # Обработка чанков аудио в реальном времени
    audio_chunk = indata.flatten()
    result = model.transcribe(audio_chunk, language="auto")
    print(result["text"])

with sd.InputStream(callback=audio_callback, channels=1, samplerate=16000):
    sd.sleep(duration=60)  # Стриминг 60 секунд
```

**Бэкенд интеграция:**
```python
# Flask API пример
from flask import Flask, request, jsonify
import whisper

app = Flask(__name__)
model = whisper.load_model("large-v3", device="cuda")

@app.route('/transcribe', methods=['POST'])
def transcribe():
    audio_file = request.files['audio']
    result = model.transcribe(audio_file, language="auto")
    return jsonify({"text": result["text"]})
```

### 2. Whisper.cpp (C/C++)

**Общая информация:**
- **Разработчик:** ggml-org
- **Лицензия:** MIT
- **Языки:** Мультиязычные модели
- **Особенности:** Оптимизированная C++ реализация

**Преимущества:**
- Очень высокая производительность
- Низкое потребление памяти
- Поддержка множества платформ
- Отличная оптимизация под CPU и GPU

**GPU поддержка:**
```bash
# Сборка с CUDA
cmake -B build -DGGML_CUDA=1
cmake --build build -j --config Release

# Запуск с GPU
./build/bin/whisper-cli -m models/ggml-large-v3.bin -f audio.wav -t 8
```

**Стриминговые возможности:**
```bash
# Реальное время с SDL2
cmake -B build -DWHISPER_SDL2=ON
cmake --build build -j --config Release
./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000
```

**Бэкенд интеграция:**
```cpp
// Пример C++ API
#include "whisper.h"

struct whisper_context * ctx = whisper_init_from_file("model.bin");
float pcm_data[16000]; // 1 секунда аудио

whisper_full(ctx, whisp, pcm_data, 16000, WHISPER_SAMPLING_RATE);
const char * text = whisper_full_get_segment_text(ctx, 0);
```

### 3. WhisperX (Python)

**Общая информация:**
- **Разработчик:** Max Bain et al.
- **Лицензия:** MIT
- **Особенности:** Расширение Whisper с выравниванием и диаризацией

**Преимущества:**
- Точные таймстампы на уровне слов
- Диаризация (разделение по дикторам)
- Поддержка пакетной обработки
- Улучшенная точность для длинных аудио

**GPU поддержка:**
```python
import whisperx

device = "cuda"
model = whisperx.load_model("large-v2", device, compute_type="float16")

# Пакетная обработка для стриминга
audio = whisperx.load_audio("audio.mp3")
result = model.transcribe(audio, batch_size=16)
```

**Стриминговые возможности:**
```python
# Стриминг с буферизацией
import whisperx
import numpy as np

model = whisperx.load_model("base", device="cuda")
buffer = []

def process_audio_chunk(chunk):
    buffer.extend(chunk)
    if len(buffer) >= 16000 * 3:  # 3 секунды буфер
        audio_array = np.array(buffer[:16000 * 3])
        result = model.transcribe(audio_array, language="auto")
        buffer = buffer[16000 * 3:]
        return result["text"]
    return ""
```

### 4. SenseVoice (FunASR)

**Общая информация:**
- **Разработчик:** Alibaba (FunASR)
- **Лицензия:** Apache 2.0
- **Особенности:** Специализированная модель для мультиязычного ASR

**Преимущества:**
- Оптимизирована для азиатских и европейских языков
- Поддержка эмоционального распознавания
- Высокая скорость обработки
- Поддержка VAD (Voice Activity Detection)

**GPU поддержка:**
```python
from funasr import AutoModel

model = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,
    device="cuda:0",
)

# Стриминговая обработка
result = model.generate(
    input="path/to/audio.wav",
    language="auto",  # автоопределение языка
    use_itn=True,
)
```

**Бэкенд интеграция:**
```python
# WebUI для стриминга
python webui.py

# Или программный API
from funasr import AutoModel

model = AutoModel(model="iic/SenseVoiceSmall", device="cuda:0")

def streaming_transcribe(audio_stream):
    for chunk in audio_stream:
        result = model.generate(chunk, language="auto")
        yield result[0]["text"]
```

### 5. Speaches AI

**Общая информация:**
- **Разработчик:** Speaches-AI
- **Лицензия:** MIT
- **Особенности:** OpenAI-совместимый API с поддержкой стриминга

**Преимущества:**
- Полностью совместим с OpenAI API
- Поддержка WebSockets для стриминга
- Docker-контейнеры для легкого развертывания
- Поддержка GPU через CUDA

**GPU поддержка:**
```bash
# Docker с CUDA
docker run \
  --rm \
  --detach \
  --publish 8000:8000 \
  --gpus=all \
  ghcr.io/speaches-ai/speaches:latest-cuda
```

**Стриминговые возможности:**
```javascript
// WebSocket клиент для стриминга
const ws = new WebSocket('ws://localhost:8000/v1/audio/transcriptions/stream');

ws.onopen = () => {
    console.log('Connected to streaming server');
};

ws.onmessage = (event) => {
    const result = JSON.parse(event.data);
    console.log('Transcription:', result.text);
};

// Отправка аудио чанками
function sendAudioChunk(audioChunk) {
    ws.send(audioChunk);
}
```

### 6. Vosk API

**Общая информация:**
- **Разработчик:** Alpha Cephei
- **Лицензия:** Apache 2.0
- **Особенности:** Офлайн распознавание с малым размером моделей

**Преимущества:**
- Очень маленький размер моделей (50-200MB)
- Полностью офлайн работа
- Поддержка 20+ языков
- Низкие системные требования

**GPU поддержка:**
```python
# Vosk в основном CPU-ориентирован
# Но есть экспериментальная поддержка GPU
from vosk import Model, KaldiRecognizer

model = Model("model-path")
recognizer = KaldiRecognizer(model, 16000)

# Обработка в реальном времени
while True:
    data = stream.read(4000)
    if len(data) == 0:
        break
    if recognizer.AcceptWaveform(data):
        result = recognizer.Result()
        print(result["text"])
```

## Сравнительная таблица решений

| Решение | Языки | GPU | Стриминг | Точность | Скорость | Размер модели | Лицензия | Бэкенд |
|----------|--------|-----|----------|----------|---------|-------------|----------|---------|
| Whisper (Python) | 99 | ✓ | Ограниченный | Высокая | Средняя | 39M-1.5B | MIT | Flask/FastAPI |
| Whisper.cpp | 99 | ✓ | ✓ | Высокая | Очень высокая | 75M-2.9G | MIT | C++ API |
| WhisperX | 99 | ✓ | ✓ | Очень высокая | Высокая | 39M-1.5B | MIT | Python API |
| SenseVoice | 10+ | ✓ | ✓ | Высокая | Очень высокая | ~500M | Apache 2.0 | WebUI/API |
| Speaches AI | 99 | ✓ | ✓ | Высокая | Высокая | 100M-1.5B | MIT | OpenAI API |
| Vosk | 20+ | ✗ | ✓ | Средняя | Высокая | 50-200M | Apache 2.0 | Мультиязычный |

## Рекомендации по выбору

### Для высоконагруженных корпоративных систем

**Рекомендация:** Whisper.cpp + кастомный бэкенд

**Обоснование:**
- Максимальная производительность
- Низкое потребление ресурсов
- Полный контроль над процессом
- Отличная масштабируемость

**Архитектура:**
```
[Микрофон] → [Буферизация] → [Whisper.cpp (GPU)] → [API Gateway] → [Клиент]
```

### Для быстрой разработки и прототипирования

**Рекомендация:** Speaches AI или WhisperX

**Обоснование:**
- Готовые API
- Простота интеграции
- Поддержка стриминга "из коробки"
- Хорошая документация

**Архитектура:**
```
[Клиент] → [WebSocket] → [Speaches AI (Docker)] → [GPU] → [База данных]
```

### Для мобильных и IoT устройств

**Рекомендация:** Vosk API

**Обоснование:**
- Маленький размер моделей
- Низкое потребление энергии
- Офлайн работа
- Поддержка ARM архитектуры

## Пример реализации стримингового сервера на Whisper.cpp

### Архитектура сервера

```cpp
// main.cpp - стриминговый сервер на Whisper.cpp
#include <whisper.h>
#include <iostream>
#include <thread>
#include <vector>
#include <mutex>
#include <queue>

class StreamingASRServer {
private:
    struct whisper_context * ctx;
    std::queue<std::vector<float>> audio_queue;
    std::mutex queue_mutex;
    bool running;
    
public:
    StreamingASRServer(const std::string& model_path) {
        ctx = whisper_init_from_file(model_path.c_str());
        running = true;
    }
    
    void add_audio_chunk(const std::vector<float>& chunk) {
        std::lock_guard<std::mutex> lock(queue_mutex);
        audio_queue.push(chunk);
    }
    
    void processing_loop() {
        while (running) {
            std::vector<float> chunk;
            {
                std::lock_guard<std::mutex> lock(queue_mutex);
                if (!audio_queue.empty()) {
                    chunk = audio_queue.front();
                    audio_queue.pop();
                }
            }
            
            if (!chunk.empty()) {
                // Обработка чанка на GPU
                whisper_full(ctx, whisp, chunk.data(), chunk.size(), WHISPER_SAMPLING_RATE);
                const char * text = whisper_full_get_segment_text(ctx, 0);
                
                // Отправка результата клиентам
                broadcast_transcription(text);
            }
            
            std::this_thread::sleep_for(std::chrono::milliseconds(10));
        }
    }
    
    void start() {
        std::thread processor(&StreamingASRServer::processing_loop, this);
        processor.detach();
    }
};
```

### Python клиент для стриминга

```python
# streaming_client.py
import asyncio
import websockets
import json
import pyaudio
import numpy as np

async def streaming_client():
    uri = "ws://localhost:8765/stream"
    
    # Настройка аудио захвата
    audio = pyaudio.PyAudio()
    stream = audio.open(format=pyaudio.paFloat32,
                    channels=1,
                    rate=16000,
                    input=True,
                    frames_per_buffer=1024)
    
    async with websockets.connect(uri) as websocket:
        print("Connected to streaming server")
        
        def audio_callback(in_data, frame_count, time_info, status):
            asyncio.create_task(send_audio_chunk(websocket, in_data))
            return (in_data, pyaudio.paContinue)
        
        stream.start_stream(audio_callback)
        
        # Прием результатов
        async for message in websocket:
            result = json.loads(message)
            print(f"Transcription: {result['text']}")
    
    stream.stop_stream()
    stream.close()
    audio.terminate()

async def send_audio_chunk(websocket, audio_data):
    await websocket.send(audio_data.tobytes())

if __name__ == "__main__":
    asyncio.run(streaming_client())
```

### Docker конфигурация

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu22.04

RUN apt-get update && apt-get install -y \
    cmake \
    build-essential \
    libssl-dev \
    portaudio19-dev \
    python3 \
    python3-pip

WORKDIR /app
COPY . .

RUN mkdir build && cd build && \
    cmake .. -DGGML_CUDA=1 && \
    make -j$(nproc)

EXPOSE 8765
CMD ["./build/streaming_server", "-m", "/models/ggml-large-v3.bin"]
```

## Оптимизация производительности

### 1. Выбор модели

```python
# Рекомендации по выбору модели в зависимости от задачи
MODEL_RECOMMENDATIONS = {
    "real_time_low_latency": {
        "model": "tiny",
        "params": {"beam_size": 1, "best_of": 1},
        "expected_latency": "< 500ms",
        "gpu_memory": "1-2GB"
    },
    "balanced_accuracy": {
        "model": "base", 
        "params": {"beam_size": 5, "best_of": 5},
        "expected_latency": "1-2s",
        "gpu_memory": "2-4GB"
    },
    "high_accuracy": {
        "model": "large-v3",
        "params": {"beam_size": 10, "best_of": 10},
        "expected_latency": "3-5s", 
        "gpu_memory": "8-16GB"
    }
}
```

### 2. Оптимизация буферизации

```python
# Адаптивная буферизация
class AdaptiveBuffer:
    def __init__(self, min_size=1024, max_size=8192):
        self.min_size = min_size
        self.max_size = max_size
        self.current_size = min_size
        self.latency_history = []
    
    def update_buffer_size(self, processing_time):
        self.latency_history.append(processing_time)
        
        if len(self.latency_history) > 10:
            avg_latency = sum(self.latency_history[-10:]) / 10
            
            if avg_latency > 2.0:  # Если задержка > 2с
                self.current_size = max(self.min_size, self.current_size * 0.8)
            elif avg_latency < 0.5:  # Если задержка < 0.5с
                self.current_size = min(self.max_size, self.current_size * 1.2)
    
    def get_buffer_size(self):
        return int(self.current_size)
```

### 3. Мультиязычное распознавание

```python
# Интеллектуальное определение языка
class LanguageDetector:
    def __init__(self):
        self.language_models = {
            "ru": whisper.load_model("base", device="cuda"),
            "en": whisper.load_model("base", device="cuda")
        }
    
    def detect_and_transcribe(self, audio_chunk):
        # Быстрая детекция языка
        ru_result = self.language_models["ru"].transcribe(
            audio_chunk, language="ru", fp16=False
        )
        en_result = self.language_models["en"].transcribe(
            audio_chunk, language="en", fp16=False  
        )
        
        # Выбор результата с большей уверенностью
        if ru_result["language_probs"]["ru"] > en_result["language_probs"]["en"]:
            return ru_result["text"], "ru"
        else:
            return en_result["text"], "en"
```

## Заключение

На основе проведенного исследования можно сделать следующие выводы:

1. **Лучшее решение для корпоративного использования:** Whisper.cpp + кастомный бэкенд
   - Максимальная производительность и контроль
   - Поддержка GPU и стриминга
   - Масштабируемость

2. **Лучшее решение для быстрой разработки:** Speaches AI
   - Готовый OpenAI-совместимый API
   - Поддержка WebSocket стриминга
   - Простота развертывания

3. **Лучшее решение для мобильных устройств:** Vosk API
   - Маленький размер моделей
   - Офлайн работа
   - Низкое потребление ресурсов

4. **Рекомендуемый стек технологий:**
   ```
   Фронтенд: React/Vue.js + WebSocket клиент
   Бэкенд: Whisper.cpp (C++) + FastAPI (Python)
   База данных: PostgreSQL/Redis для кэширования
   Инфраструктура: Docker + Kubernetes
   GPU: NVIDIA CUDA 12.x
   ```

5. **Ключевые факторы успеха:**
   - Правильный выбор размера модели
   - Оптимизация буферизации
   - Эффективное использование GPU
   - Масштабируемая архитектура
   - Качественная предобработка аудио

Данное исследование показывает, что современные решения для мультиязычного распознавания речи достигли высокого уровня зрелости и готовы для использования в производственных системах с поддержкой стриминга и GPU-ускорения.